<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.335">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Suraj Kiron Nair">
<meta name="dcterms.date" content="2023-03-05">

<title>Portfolio - Vision Based Pose and Velocity Estimation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Portfolio</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../resume.pdf">
 <span class="menu-text">Resume</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/surajkiron"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/suraj-kiron/"><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Vision Based Pose and Velocity Estimation</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Computer Vision</div>
                <div class="quarto-category">Visual Odometry</div>
                <div class="quarto-category">MATLAB</div>
                <div class="quarto-category">Quadrotors</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Suraj Kiron Nair </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 5, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="overview" class="level2">
<h2 class="anchored" data-anchor-id="overview">Overview</h2>
<p>This project is composed of 2 parts. In part one we have to determine the position and orientation of a quadrotor flying over a Mat of April Tags. For the second part, I estimate the linear and angular velocity of the drone using Optical Flow.</p>
<p>The data for this project was collected using a Nano+ quadrotor that was either held by hand or flown through a prescribed trajectory over a mat of AprilTags, each of which has a unique ID.Thes images captured by the quadrotor are in a time sequence. We know the coordinates of each point on the April Tag mat. Using this information we can find the position and orientation of the quadrotor in the world frame.</p>
<p>The intrinsic camera calibration matrix and the transformation between the camera and the robot center are known. These Two photos are included to visualize the camera-robot body transform</p>
<div id="fig-elephants" class="quarto-layout-panel">
<figure class="figure">
<div class="quarto-layout-row quarto-layout-valign-top">
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-surus" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="side_view.jpg" class="img-fluid figure-img" data-ref-parent="fig-elephants"></p>
<p></p><figcaption class="figure-caption">(a) Side View</figcaption><p></p>
</figure>
</div>
</div>
<div class="quarto-layout-cell quarto-layout-cell-subref" style="flex-basis: 50.0%;justify-content: center;">
<div id="fig-hanno" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="top_view.jpeg" class="img-fluid figure-img" data-ref-parent="fig-elephants"></p>
<p></p><figcaption class="figure-caption">(b) Top View</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p></p><figcaption class="figure-caption">Figure&nbsp;1: Camera to Body transformation</figcaption><p></p>
</figure>
</div>
<p>The data contains a struct array of image data called data, which holds all of the data necessary to do pose estimation. This includes the time stamps, April Tag IDs observed for each time stamp and the location of the April Tag corners and centers in image coordinates.</p>
<p>This is a rectified image of the April Tag Mat.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="aprilTagMat.png" class="img-fluid figure-img" style="width:50.0%"></p>
<p></p><figcaption class="figure-caption">April Tag Mat</figcaption><p></p>
</figure>
</div>
</section>
<section id="part-i-pose-estimation" class="level2">
<h2 class="anchored" data-anchor-id="part-i-pose-estimation">Part I: Pose Estimation</h2>
<section id="methodology" class="level3">
<h3 class="anchored" data-anchor-id="methodology">Methodology</h3>
<p>The goal of this section is to use this data to determine the pose of the quadrotor. I have to use the points of the corners of each AprilTag along with their corresponding positions in the image to estimate the pose of the camera, and then the drone body. I do this by computing the homography matrix H for the points whose position we know in both the image plane as well as the real-world. The homography matrix converts points in the 3D world frame to the 2D image plane. Here, all the real-world points have Z coordinate as 0. Hence:</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Picture1.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>We get the Projective transformation equation as</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Picture2.png" class="img-fluid figure-img"></p>
</figure>
</div>
<p>We need 4 correspondences to constrain all 8 degrees of freedom. The elements are stored in row order In general, a Projective transformation can map any 4 points to any 4 points, with no triplets of collinear points. However, using more than 4 correspondences makes the results more robust. Stacking together all correspondences for a given time stamp we get the <span class="math inline">\(A\)</span> matrix. Hence,</p>
<p><span class="math display">\[Ah = 0\]</span></p>
<p>We perform least squares optimization by taking the SVD decomposition of matrix A.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Picture3.png" class="img-fluid figure-img" style="width:35.0%"></p>
</figure>
</div>
<p>We get the homography matrix from 9th coloumn of the V matrix.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Picture4.png" class="img-fluid figure-img" style="width:80.0%"></p>
</figure>
</div>
<p>From this we extract our Rotation matrix <span class="math inline">\((R)\)</span> and our Translation Vector <span class="math inline">\((T)\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Picture5.png" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
<p>The diagonal guarantees it is a rotation matrix with determinant 1. To find our estimate of the translation we just make sure it is in the right scale using the following.</p>
<p><span class="math display">\[T = \hat{T}/||\hat{R_1}||\]</span></p>
</section>
</section>
<section id="pose-tracking-results" class="level2">
<h2 class="anchored" data-anchor-id="pose-tracking-results">Pose Tracking Results</h2>
<p>The results show that the calculated pose (blue) is closely tracking the ground truth from the Vicon (red).</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Picture6.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 1: Position and Orientation Tracking Results</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Picture7.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Dataset 2: Position and Orientation Tracking Results</figcaption><p></p>
</figure>
</div>
</section>
<section id="part-ii-velocity-estimation" class="level2">
<h2 class="anchored" data-anchor-id="part-ii-velocity-estimation">Part II: Velocity Estimation</h2>
<section id="methodology-1" class="level3">
<h3 class="anchored" data-anchor-id="methodology-1">Methodology</h3>
<p>We first extract corners in each image. I used MATLAB’s built in corner detectors detectFASTfeatures to accomplish this. The extracted corners includes corners from the random scribble.</p>
<p>After extracting corners, the next step is to compute the motion(Optical Flow) between these corners in two consecutive images. This is achieved using the Kanade-Lucas-Tomasi (KLT) feature tracker in MATLAB. Optical flow refers to the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and the scene.</p>
<p>We need to solve the Motion Field Equation for the case of fixed depth. The Motion Field Equation relates the image plane velocities of the points (denoted by <span class="math inline">\(( \dot{p} )\)</span>) to the motion of the camera or object. This equation is based on the assumption of a fixed depth (denoted by <span class="math inline">\(( Z )\)</span>), where$ ( A(p) )$ and <span class="math inline">\(( B(p) )\)</span> are functions of the point coordinates in the image, and <span class="math inline">\(( V )\)</span> and <span class="math inline">\(( \Omega )\)</span> represent the linear and angular velocities, respectively. <span class="math display">\[ \dot{p} = \frac{1}{Z} A(p)V + B(p)\Omega\]</span> <span class="math display">\[\dot{p} = \frac{1}{Z} A(p)V + B(p)\Omega = \left( \frac{1}{Z} A(p) \quad B(p) \right) \begin{pmatrix} V \\ \Omega \end{pmatrix}\]</span></p>
<p><strong>Least Squares Minimization</strong>: The estimated velocities are obtained by solving an optimization problem, where the sum of the squared differences between the observed image plane velocities (( _i )) and those predicted by the Motion Field Equation is minimized. This is a classic least squares problem</p>
<p><span class="math display">\[V^*, \Omega^* = \arg\min_{V,\Omega} \sum_{i=1}^n \left\| \left( \frac{1}{Z_i} A(p_i) \quad B(p_i) \right) \begin{pmatrix} V \\ \Omega \end{pmatrix} - \dot{p}_i \right\|^2
\]</span></p>
<p><strong>Velocity Estimation Equation</strong>: Finally, the solution to the optimization problem yields the estimated linear and angular velocities <span class="math inline">\(( V^*)\)</span> and <span class="math inline">\(( \Omega^* )\)</span>, encapsulated in a matrix form. The matrix $ ( H^+ )$ is the pseudo-inverse of a matrix <span class="math inline">\(( H )\)</span>, which relates the observed velocities <span class="math inline">\(( \dot{p} )\)</span> to the motion parameters <span class="math inline">\(( V )\)</span> and <span class="math inline">\(( \Omega )\)</span>. The pseudo-inverse is used when the system of equations is either underdetermined or overdetermined, allowing for a least squares solution to the problem.</p>
<p><span class="math display">\[\begin{pmatrix}
V^* \\
\Omega^*
\end{pmatrix} = H^+ \dot{p}\]</span></p>
</section>
<section id="ransac-random-sample-consensus" class="level3">
<h3 class="anchored" data-anchor-id="ransac-random-sample-consensus">RANSAC (RANdom SAmple Consensus)</h3>
<p>RANSAC is an algorithm used for estimating parameters of a mathematical model from a set of data that may contain outliers. The algorithm works by iteratively selecting random subsets of data points and fitting a model to these subsets. The model parameters are then evaluated on the remaining data points, and if the model fits well to a sufficient number of points, it is considered a good fit and the algorithm terminates. The number of attempts(k) required to achieve a probability of success Psuccess is given by the equation <span class="math display">\[k = \frac{\log(1 - P_{\text{success}})}{\log(1 - e^{-M})}
\]</span></p>
<p>To implement RANSAC I use the following pseudocode <img src="Picture8.png" class="img-fluid" data-fig-align="center" alt="RANSAC pseudocode"></p>
</section>
<section id="veloctiy-estimation-results" class="level3">
<h3 class="anchored" data-anchor-id="veloctiy-estimation-results">Veloctiy Estimation Results</h3>
<p>The graph has a noticeable pattern of increased activity, particularly spikes, which could indicate rapid changes in altitude or disturbances affecting the drone’s movement. The predictions seems to be noisy need to experiment by tuning tracker parameters and using different tracker algorithms. Overall, the predictions track the true velocities closely but there is room for improvement.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="Picture9.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Linear and Angular Velocity Estimation Results</figcaption><p></p>
</figure>
</div>
<p><a href="../../posts/6_RLAN_3_UKF/index.html">Next Project: Unscented Kalman Filter</a></p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>