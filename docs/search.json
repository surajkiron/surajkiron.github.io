[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "Virtual Maze Exploration and Navigation\n\n\n\n\n\n\nSLAM\n\n\nComputer Vision\n\n\nVisual Odometry\n\n\nPython\n\n\nLocalization\n\n\n\n\n\n\n\n\n\nJul 20, 2024\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n\n\n\n\n\n\nL1 Augmented Geometric Controller\n\n\n\n\n\n\nc++\n\n\nROS\n\n\nControl Systems\n\n\n\n\n\n\n\n\n\nJul 9, 2024\n\n\nSuraj Nair\n\n\n\n\n\n\n\n\n\n\n\n\nSCARA Manipulator: Kinematic Control Part II\n\n\n\n\n\n\nManipulators\n\n\nControl Systems\n\n\nMATLAB\n\n\n\n\n\n\n\n\n\nJun 26, 2024\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n\n\n\n\n\n\nSCARA Manipulator: Kinematic Control Part I\n\n\n\n\n\n\nManipulators\n\n\nControl Systems\n\n\nMATLAB\n\n\n\n\n\n\n\n\n\nJun 25, 2024\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n\n\n\n\n\n\nState Estimation with Unscented Kalman Filter\n\n\n\n\n\n\nState Estimation\n\n\nLocalization\n\n\nMATLAB\n\n\n\n\n\n\n\n\n\nMay 20, 2023\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n\n\n\n\n\n\nVision Based Pose and Velocity Estimation\n\n\n\n\n\n\nComputer Vision\n\n\nLocalization\n\n\nMATLAB\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n\n\n\n\n\n\nState Estimation with Extended Kalman Filter\n\n\n\n\n\n\nLocalization\n\n\nMATLAB\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n\n\n\n\n\n\nRace Circuit Line Follower\n\n\n\n\n\n\nControl Systems\n\n\nComputer Vision\n\n\nPython\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nSuraj K Nair\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first blog post. Welcome!\n\nTo me robotics is a fascinating discipline that combines engineering, computer science, and artificial intelligence. ie its the coolest thing ever. As a member of the Agile Robotics and Perception Lab(ARPL) at NYU I have the opportunity to work with this cutting edge technology.\nI will be using this blog to post about projects I’m working on or even random things I find interesting. I have a lot of plans in store for this summer, so I am writing this blog as a record of everything I’m doing. I’m halfway through my Masters in Mechatronics and Robotics and this blog has been a long time coming.\nI believe that robotics has the power to revolutionize the way we live, work, and interact with the world around us. Through this blog, I wish to share my passion for robotics and record my journey through this vast field.\nSo, buckle up and get ready to embark on a thrilling journey into the realm of robotics. Let’s explore the limitless possibilities and discover the future that robots hold for us all."
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#background",
    "href": "posts/1_Line_ Follower/index.html#background",
    "title": "Race Circuit Line Follower",
    "section": "Background:",
    "text": "Background:\nThe goal of this project was to perform a PID reactive control capable of following the line painted on a racing circuit. PID control is one of the fundamental concepts of Linear control systems. The Controller continuously calculates an error value as the difference between desired output and the current output and applies a correction based on proportional, integral and derivative terms(denoted by P, I, D respectively). The control signal u[k] for a PID controller can be expressed as follows. \n\nProportional:\nProportional Controller gives an output which is proportional to the current error. The error e[k] is multiplied with a proportional gain(Kp) to get the output. And hence, is 0 if the error is 0.In this case the error is the difference between the center of the image and the centroid of the racing line.\n\n\nIntegral:\nIntegral Controller provides a necessary action to eliminate the offset error which is accumulated by the P Controller.It integrates the error over a period of time until the error value reaches to zero.\n\n\nDerivative:\nDerivative Controller gives an output depending upon the rate of change or error with respect to time. It gives the kick start for the output thereby increasing system response.\nThe integral and dervivative errors are calculated as follows."
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#tuning-the-pid-controller.",
    "href": "posts/1_Line_ Follower/index.html#tuning-the-pid-controller.",
    "title": "Race Circuit Line Follower",
    "section": "Tuning the PID controller.",
    "text": "Tuning the PID controller.\nFirstly, we must setup the P controller as per Ziegler Nichols method. Keep adjusting the value of the constant, till we get a value where there occurs it has neither unstable oscillations and nor slow response.   \nOnce you get oscillations of constant amplitude you can adjust the derivative gains (Kd). After this the vehicle was much more stable and tracked the line accurately. Finally I modified the Integral gain which I found to have minimal effect on the system. But nonetheless helped to avoid any steady state errors."
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#results-and-code",
    "href": "posts/1_Line_ Follower/index.html#results-and-code",
    "title": "Race Circuit Line Follower",
    "section": "Results and Code",
    "text": "Results and Code\n\nFinal Code"
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#learning",
    "href": "posts/1_Line_ Follower/index.html#learning",
    "title": "Race Circuit Line Follower",
    "section": "Learning",
    "text": "Learning\nOne mistake I made was to set the velocity of the car too high. Since the cycle time of the system is only 12Hz the controller could not detect the change in error fast enough. This led to understeer and multiple head on collision with the track walls. It’s always a good idea to start with the minimal speed requirement when programming a controller and increase it once the system is more robust."
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#references",
    "href": "posts/1_Line_ Follower/index.html#references",
    "title": "Race Circuit Line Follower",
    "section": "References:",
    "text": "References:\nJde Robotics Visual Line Follow What is PID control? MATLAB Discrete time equations for PID control"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a student pursuing my Masters in Mechatronics and Robotics at NYU Tandon.\n\n\nNYU Tandon Institute of Technology | New York  MS Mechatronics and Robotics | Sept 2022 - May 2024\nRamaiah Institute of Technology | Bengaluru | India  BE Mechanical Engineering | Sept 2015 - June 2021\n\n\n\nAgile Robotics and Perception Lab (ARPL) | Research Assistant | Jan 2023 - present \n\nImplemented L1 controller, for my Master’s Project\nWorked on Modalai Voxl2 and Nvidia Orin based quadrotor platforms\nDeveloped C++, ROS/ROS2 packages\nFlight tested quadrotors using Vicon and GPS\nSystem Integration and testing\n\nICER IISc | Research Assistant | June 2021 - May 2022"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "NYU Tandon Institute of Technology | New York  MS Mechatronics and Robotics | Sept 2022 - May 2024\nRamaiah Institute of Technology | Bengaluru | India  BE Mechanical Engineering | Sept 2015 - June 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "ARPL | Graduate Engineer | Jan 2023 - present\nICER IISc | Research Assistant | June 2021 - May 2022"
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html",
    "href": "posts/4_RLAN_1_EKF/index.html",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "",
    "text": "In this project I use a Extended Kalman Filter to Localize a quadrotor. I use the body frame acceleration and angular velocity from the onboard IMU as your control inputs. The measurement will be given by the pose or velocity from the Vicon. Vicon data is in the following format: \\[ [x, y, z, roll, pitch, yaw, v_x, v_y, v_z, \\omega_x, \\omega_y, \\omega_z]^T \\]\nThe on board processor of the robot collects synchronized camera and IMU data and sends them to the mission computer. At this stage, the camera data should not be used. The sensor data is decoded into standard MATLAB format. Note that since the sensor data is transmitted via wireless network, there may or may not be a sensor packet available during a specific iteration of the control loop. A sensor packet is a struct that contains following fields:\nThe goal is to use an Extended Kalman Filter (EKF) to estimate the position, velocity, and orientation, and sensor biases of an Micro Aerial Vehicle. The Vicon velocity is given in the world frame, whereas the angular rate in the body frame of the robot. Furthermore, I use the body frame acceleration and angular velocity from the on board IMU as the inputs.\nI have implemented 2 versions of the filter. In the first one, the measurement update is given by the position and orientation from vicon, in the second one I use only the velocity from the Vicon.. In both parts, the process model is the same."
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#assumptions",
    "href": "posts/4_RLAN_1_EKF/index.html#assumptions",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "Assumptions",
    "text": "Assumptions\nWe make the assumption that the noise in the readings obtained from the IMU and Vicon adhere to a normal distribution. Additionally, we can assume the state derivative to be both continuous and differentiable, allowing us to linearize it. Based on these assumptions, we can utilize the EKF algorithm for predicting the state."
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#the-process-model",
    "href": "posts/4_RLAN_1_EKF/index.html#the-process-model",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "The Process Model: ",
    "text": "The Process Model: \nThe state \\(\\mathbf{X}\\) is given by \\[\\mathbf{X} =\n\\begin{bmatrix}\n\\mathbf{x_1} \\\\\n\\mathbf{x_2} \\\\\n\\mathbf{x_3} \\\\\n\\mathbf{x_4} \\\\\n\\mathbf{x_5}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{p} \\\\\n\\mathbf{q} \\\\\n\\mathbf{\\dot{p}} \\\\\n\\mathbf{b_g} \\\\\n\\mathbf{b_a}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{position} \\\\\n\\mathbf{orientation} \\\\\n\\mathbf{linear \\, velocity} \\\\\n\\mathbf{gyroscope \\, bias} \\\\\n\\mathbf{accelerometer \\, bias}\n\\end{bmatrix}\n\\]\nHere \\(\\mathbf{X} \\in \\mathbf{R}^{15}\\) and \\(\\mathbf{q} = [\\phi, \\theta, \\psi] ^T = [roll, pitch, yaw]^T\\). Using the properties of the distributions we can derive the state transition equations.\nWe already have \\(\\dot{\\mathbf{p}}\\) in the state vector. We can calculate \\(\\dot{\\mathbf{q}}\\) using the angular velocity measured by IMU \\((\\omega_m)\\) which is expressed in body frame. Since the gyroscope noise is additive white noise \\(n_g \\sim \\mathit{N}(0,\\mathit{Q})\\). \\[\n\\omega_m = \\omega + b_g + n_g\n\\] \\[\n\\omega = \\omega_m - b_g - n_g\n\\] \\(G(q)\\) is a transformation that maps the euler angle derivatives \\((\\dot{q} = [\\dot{\\phi}, \\dot{\\theta}, \\dot{\\psi}]^T)\\) to angular velocity expressed in world frame. Where \\(G(\\mathbf{q})^{-1}\\) maps orientation to angular velocity and is given by. See eqn 2.76 here \\[\nG(\\mathbf{q})^{-1} =\n\\begin{bmatrix}\n\\frac{\\cos(z)\\sin(y)}{\\cos(y)} & \\frac{\\sin(z)\\sin(y)}{\\cos(y)} & 1\\\\\n-\\sin(z) & \\cos(z) & 0\\\\\n\\frac{\\cos(z)}{\\cos(y)} & \\frac{\\sin(z)}{\\cos(y)} & 0\n\\end{bmatrix}\n\\] \\[\n(\\omega)_{world} = G(q) \\dot{q}\n\\] Since \\(\\omega\\) is expressed in the body frame it need to be rotated to be expressed with respect to the world using \\(R^w_b\\). \\[R^w_b \\omega = G(q) \\dot{q}\\]\nHence finally we get the euler angle derivatives to be \\[\\dot{q} = G(q)^{-1}R_b^w \\omega\\] \\[\\dot{q} = G(q)^{-1}R_b^w(\\omega_m - b_g - n_g) \\]\nSimilarly for acceleration, since we have additive accelerometer noise. \\(n_a \\sim \\mathit{N}(0,\\mathit{Q})\\) The measured acceleration is given by. \\[a_m = R_b^w(\\ddot{p} - g) + b_a + n_a \\]\nand let \\(\\dot{b_g} = n_{bg}\\) and \\(\\dot{b_a} = n_{ba}\\) be gyroscope noise and accelerometer white noise respectively.\nDifferentiating we get the state transition equation as \\[\n\\mathbf{\\dot{X}}=\n\\begin{bmatrix}\n\\mathbf{\\dot{p}} \\\\\n\\mathbf{\\dot{q}} \\\\\n\\mathbf{\\ddot{p }} \\\\\n\\mathbf{\\dot{b_g}} \\\\\n\\mathbf{\\dot{b_a}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n\\mathbf{x_3} \\\\\nG(\\mathbf{q})^{-1}R_b^w (\\mathbf{\\omega_m} - \\mathbf{b_g} - \\mathbf{n_g})\\\\\n\\mathbf{g}+R_b^w(\\mathbf{a_m}-\\mathbf{b_a}-\\mathbf{n_a}) \\\\\n\\mathbf{n_{bg}} \\\\\n\\mathbf{n_{ba}}\n\\end{bmatrix}\n=\nf(\\mathbf{X}, \\mathbf{u}, \\mathbf{n})\n\\]"
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#prediction-step",
    "href": "posts/4_RLAN_1_EKF/index.html#prediction-step",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "Prediction Step:",
    "text": "Prediction Step:\n\n\n\nPrediction Equations\n\n\nWe use MATLAB symbolic library to determine \\(A_t\\) and \\(U_t\\). \\(Q_d\\) is the covariance of the IMU noise. In the prediction stage, the filter uses the system model to make a prediction of the next state of the system based on the current state and any control inputs from the IMU"
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#update-step",
    "href": "posts/4_RLAN_1_EKF/index.html#update-step",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "Update Step:",
    "text": "Update Step:\nThe prediction is used to estimate the measurement that will be obtained at the next time step. During the correction stage, the filter incorporates both the predicted measurement and the actual measurement taken at the following time step to refine its assessment of the system’s state. The refined estimate is a weighted sum of the predicted state and the actual measurement, with the filter’s assessment of the uncertainty in both the model and the measurements used to determine the weighting of each component using the Kalman Gain.\n\n\n\nUpdate Equations\n\n\nHere the observation model \\(\\mathbf{z}\\) is given by \\[\n\\mathbf{z}=\n\\begin{bmatrix}\n\\mathbf{p} \\\\\n\\mathbf{q} \\\\\n\\mathbf{r}\n\\end{bmatrix}\n+\n\\mathbf{v}\n=\n\\mathbf{C_t X}+\\mathbf{v}\n\\]\nWhere \\(C_t\\) is a selection matrix. For case 1 we select position and orientation \\[\nC_t =\n\\begin{bmatrix}\nI & 0 & 0 & 0 & 0 & 0\\\\\n0 & I & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nFor case 2 we select Linear velocitiy \\[\nC_t =\n\\begin{bmatrix}\n0 & 0 & I & 0 & 0 & 0\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#results",
    "href": "posts/4_RLAN_1_EKF/index.html#results",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "Results",
    "text": "Results\nI have compared both implementations of the Extended Kalman filter on 3 datasets. 3 being the most agressive flight.\n\nCase 1 Dataset 1\n\n\n\nState Estimation Dataset 1\n\n\n\n\nCase 1 Dataset 2\n\n\n\nState Estimation Dataset 2\n\n\n\n\nCase 1 Dataset 3\n\n\n\nState Estimation Dataset 3\n\n\n\n\nCase 2 Dataset 1\n\n\n\nState Estimation Dataset 1\n\n\n\n\nCase 2 Dataset 2\n\n\n\nState Estimation Dataset 2: shows yaw drift\n\n\n\n\nCase 2 Dataset 3\n\n\n\nState Estimation Dataset 3: shows yaw drift\n\n\nThe results show that the yaw estimate of the quadrotor drifts if we use only velocity measurements to update the EKF. Measuring pose gives us better tracking performance.\nNext Project: Vision Based Pose and Velocity Estimation"
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html",
    "href": "posts/5_RLAN_2_VO/index.html",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "",
    "text": "This project is composed of 2 parts. In part one we have to determine the position and orientation of a quadrotor flying over a Mat of April Tags. For the second part, I estimate the linear and angular velocity of the drone using Optical Flow.\nThe data for this project was collected using a Nano+ quadrotor that was either held by hand or flown through a prescribed trajectory over a mat of AprilTags, each of which has a unique ID.Thes images captured by the quadrotor are in a time sequence. We know the coordinates of each point on the April Tag mat. Using this information we can find the position and orientation of the quadrotor in the world frame.\nThe intrinsic camera calibration matrix and the transformation between the camera and the robot center are known. These Two photos are included to visualize the camera-robot body transform\n\n\n\n\n\n\n\n\n\n\n\n(a) Side View\n\n\n\n\n\n\n\n\n\n\n\n(b) Top View\n\n\n\n\n\n\n\nFigure 1: Camera to Body transformation\n\n\n\nThe data contains a struct array of image data called data, which holds all of the data necessary to do pose estimation. This includes the time stamps, April Tag IDs observed for each time stamp and the location of the April Tag corners and centers in image coordinates.\nThis is a rectified image of the April Tag Mat.\n\n\n\nApril Tag Mat"
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html#part-i-pose-estimation",
    "href": "posts/5_RLAN_2_VO/index.html#part-i-pose-estimation",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "Part I: Pose Estimation",
    "text": "Part I: Pose Estimation\n\nMethodology\nThe goal of this section is to use this data to determine the pose of the quadrotor. I have to use the points of the corners of each AprilTag along with their corresponding positions in the image to estimate the pose of the camera, and then the drone body. I do this by computing the homography matrix H for the points whose position we know in both the image plane as well as the real-world. The homography matrix converts points in the 3D world frame to the 2D image plane. Here, all the real-world points have Z coordinate as 0. Hence:\n\n\n\n\n\nWe get the Projective transformation equation as\n\n\n\n\n\nWe need 4 correspondences to constrain all 8 degrees of freedom. The elements are stored in row order In general, a Projective transformation can map any 4 points to any 4 points, with no triplets of collinear points. However, using more than 4 correspondences makes the results more robust. Stacking together all correspondences for a given time stamp we get the \\(A\\) matrix. Hence,\n\\[Ah = 0\\]\nWe perform least squares optimization by taking the SVD decomposition of matrix A.\n\n\n\n\n\nWe get the homography matrix from 9th coloumn of the V matrix.\n\n\n\n\n\nFrom this we extract our Rotation matrix \\((R)\\) and our Translation Vector \\((T)\\).\n\n\n\n\n\nThe diagonal guarantees it is a rotation matrix with determinant 1. To find our estimate of the translation we just make sure it is in the right scale using the following.\n\\[T = \\hat{T}/||\\hat{R_1}||\\]"
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html#pose-tracking-results",
    "href": "posts/5_RLAN_2_VO/index.html#pose-tracking-results",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "Pose Tracking Results",
    "text": "Pose Tracking Results\nThe results show that the calculated pose (blue) is closely tracking the ground truth from the Vicon (red).\n\n\n\nDataset 1: Position and Orientation Tracking Results\n\n\n\n\n\nDataset 2: Position and Orientation Tracking Results"
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html#part-ii-velocity-estimation",
    "href": "posts/5_RLAN_2_VO/index.html#part-ii-velocity-estimation",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "Part II: Velocity Estimation",
    "text": "Part II: Velocity Estimation\n\nMethodology\nWe first extract corners in each image. I used MATLAB’s built in corner detectors detectFASTfeatures to accomplish this. The extracted corners includes corners from the random scribble.\nAfter extracting corners, the next step is to compute the motion(Optical Flow) between these corners in two consecutive images. This is achieved using the Kanade-Lucas-Tomasi (KLT) feature tracker in MATLAB. Optical flow refers to the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and the scene.\nWe need to solve the Motion Field Equation for the case of fixed depth. The Motion Field Equation relates the image plane velocities of the points (denoted by \\(( \\dot{p} )\\)) to the motion of the camera or object. This equation is based on the assumption of a fixed depth (denoted by \\(( Z )\\)), where$ ( A(p) )$ and \\(( B(p) )\\) are functions of the point coordinates in the image, and \\(( V )\\) and \\(( \\Omega )\\) represent the linear and angular velocities, respectively. \\[ \\dot{p} = \\frac{1}{Z} A(p)V + B(p)\\Omega\\] \\[\\dot{p} = \\frac{1}{Z} A(p)V + B(p)\\Omega = \\left( \\frac{1}{Z} A(p) \\quad B(p) \\right) \\begin{pmatrix} V \\\\ \\Omega \\end{pmatrix}\\]\nLeast Squares Minimization: The estimated velocities are obtained by solving an optimization problem, where the sum of the squared differences between the observed image plane velocities (( _i )) and those predicted by the Motion Field Equation is minimized. This is a classic least squares problem\n\\[V^*, \\Omega^* = \\arg\\min_{V,\\Omega} \\sum_{i=1}^n \\left\\| \\left( \\frac{1}{Z_i} A(p_i) \\quad B(p_i) \\right) \\begin{pmatrix} V \\\\ \\Omega \\end{pmatrix} - \\dot{p}_i \\right\\|^2\n\\]\nVelocity Estimation Equation: Finally, the solution to the optimization problem yields the estimated linear and angular velocities \\(( V^*)\\) and \\(( \\Omega^* )\\), encapsulated in a matrix form. The matrix $ ( H^+ )$ is the pseudo-inverse of a matrix \\(( H )\\), which relates the observed velocities \\(( \\dot{p} )\\) to the motion parameters \\(( V )\\) and \\(( \\Omega )\\). The pseudo-inverse is used when the system of equations is either underdetermined or overdetermined, allowing for a least squares solution to the problem.\n\\[\\begin{pmatrix}\nV^* \\\\\n\\Omega^*\n\\end{pmatrix} = H^+ \\dot{p}\\]\n\n\nRANSAC (RANdom SAmple Consensus)\nRANSAC is an algorithm used for estimating parameters of a mathematical model from a set of data that may contain outliers. The algorithm works by iteratively selecting random subsets of data points and fitting a model to these subsets. The model parameters are then evaluated on the remaining data points, and if the model fits well to a sufficient number of points, it is considered a good fit and the algorithm terminates. The number of attempts(k) required to achieve a probability of success Psuccess is given by the equation \\[k = \\frac{\\log(1 - P_{\\text{success}})}{\\log(1 - e^{-M})}\n\\]\nTo implement RANSAC I use the following pseudocode \n\n\nVeloctiy Estimation Results\nThe graph has a noticeable pattern of increased activity, particularly spikes, which could indicate rapid changes in altitude or disturbances affecting the drone’s movement. The predictions seems to be noisy need to experiment by tuning tracker parameters and using different tracker algorithms. Overall, the predictions track the true velocities closely but there is room for improvement.\n\n\n\nLinear and Angular Velocity Estimation Results\n\n\nNext : Unscented Kalman Filter"
  },
  {
    "objectID": "posts/7_Manipulator_Kinematics/index.html",
    "href": "posts/7_Manipulator_Kinematics/index.html",
    "title": "SCARA Manipulator: Kinematic Control Part I",
    "section": "",
    "text": "This project will cover the following topics:  1. Deriving Forward kinematics using DH Parmamertization of a robotic Arm  2. Forward Differential Kinematics  3. Inverse Differential Kinematics (using Jacobian Inverse and Jacobian Transpose) 4. Exploiting redundant DOFs to add a secondary objective\nConsider the SCARA manipulator depicted below.\n\n\n\n\n\nOur Goal is to have the SCARA manipulator end effector follow the given position and velocity trajectories.\n\nThe manipulator parameteres are \\[ d_0 = 1 \\: m\\] \\[a_1 = a_2 = 0.5 \\: m\\] \\[ \\theta_{1_{min}} = - \\pi / 2 \\: rad, \\theta_{1_{max}} = \\pi / 2 \\: rad \\] \\[ \\theta_{2_{min}} = - \\pi / 2 \\: rad, \\theta_{2_{max}} =  \\pi / 4 \\: rad  \\] \\[ d_{3_{min}} = 0.25\\: m , d_{3_{max}} = 1\\: m \\] \\[ \\theta_{4_{min}} = - 2\\pi \\: rad, \\theta_{4_{max}} =  2\\pi \\: rad  \\]\nThe frames are depicted into the figure and the DH parameters are:\n\nDH Parameters\n\n\n\n\n\n\n\n\n\n\n\\(d_i\\)\n\\(\\alpha_i\\)\n\\(\\theta_i\\)\n\\(a_i\\)\n\n\n\n\nLink 1\n\\(0\\)\n\\(0\\)\n\\(\\theta_1\\)\n\\(a_1\\)\n\n\nLink 2\n\\(0\\)\n\\(0\\)\n\\(\\theta_2\\)\n\\(a_2\\)\n\n\nLink 3\n\\(d_3\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\nLink 4\n\\(0\\)\n\\(0\\)\n\\(\\theta_4\\)\n\\(0\\)\n\n\n\nwhere \\(d_i\\) and \\(\\alpha\\) are the translational and rotational offsets along the \\(x\\) axis of frame \\(i-1\\) and frame \\(i\\). \\(\\theta_i\\) and \\(a_i\\) are the joint angles and link lengths respectively.\nNote that the 0 frame is not coincident with the b frame. There is a translation from the ground plane denoted with \\(d_0 = 1\\). The frame 4 is coincident with the frame 3 at the starting. Be careful on the \\(d3\\) component. The range of values is always positive. When the arm is fully extended (down towards the floor) the value is 1m whereas 0.25 when retracted (away from the floor). However, when you build your matrix note that \\(d_3\\) moves along \\(−z_2\\) axis and for this reason you translation in \\(A_{23}\\) should be negative as \\(−d_3\\)."
  },
  {
    "objectID": "posts/7_Manipulator_Kinematics/index.html#part-1",
    "href": "posts/7_Manipulator_Kinematics/index.html#part-1",
    "title": "SCARA Manipulator: Kinematic Control Part I",
    "section": "Part 1",
    "text": "Part 1\nWe implement the algorithms for kinematic inversion with inverse and jacobian transpose along the given trajectory. Adopting the Euler integration rule with integration time 1 ms. Implement a final function visualize results for each part (joint value and error).\nThe inverse kinematics equation for the given manipulator is given by \\[ \\dot{q} = J^{-1}_a(q)(\\dot{x}_d + Ke)\\] Where ‘ is the inverse of the Jacobian. is the velocity trajectory. ’ is the error between the given trajectory and the end effector position and is the gain of the system. For the SCARA robot the Jacobian is a 4×4 matrix ( Linear velocities along x, y and z and one orientation). Since there is only one orientation component the Geometric Jacobian is equal to the Analytical Jacobian. The equation given above is modeled using the following kinematic control schematic diagram.\n\n\n\n\nSimulink Block Diagram\n\n\n\nHere the ‘From Workspace’ block is used to import time series trajectory data from the workspace. A subtract block is used to calculate the error between the required trajectory and the actual position of the end effector which is calculated using direct kinematics. The error signal is passed through a gain block. The gain is set to 1000.\nFurther, an addition block and product block is used to complete the right hand side of the equation. A discrete integrator block is used to obtain from . is the input to two functions.\n\n\nDerive the Direct Kinematics\nThe direct kinematics of the system can be represented by the equation \\[ \\mathbf{x = K(q)}\\] where \\[\\mathbf{q} =\n    \\begin{bmatrix}\n    \\theta_1 \\\\\n    \\theta_2 \\\\\n    d_3 \\\\\n    \\theta_4\n    \\end{bmatrix}\n\\] and \\(\\mathbf{K(q)}\\) is a tranformation that maps the joint space \\(\\mathbf{q}\\) to the operation space \\(\\mathbf{x}\\). We can find \\(\\mathbf{K(q)}\\) by composing all the transformations from the base frame to the end effector frame.\nThe tranformation \\(A_b^0\\) is a pure translation along \\(z_b\\) \\[ A_b^0 =\n    \\begin{bmatrix}\n    1 & 0 & 0 & 0\\\\\n    0 & 1 & 0 & 0\\\\\n    0 & 0 & 1 & d_0\\\\\n    0 & 0 & 0 & 1\n    \\end{bmatrix},\nA_0^1 =\n    \\begin{bmatrix}\n    \\cos(\\theta_1) & -\\sin(\\theta_1) & 0 & a_1 \\cos(\\theta_1)\\\\\n    \\sin(\\theta_1) & \\cos(\\theta_1) & 0 & a_1\\sin(\\theta_1)\\\\\n    0 & 0 & 1 & 0\\\\\n    0 & 0 & 0 & 1\n    \\end{bmatrix}\n\\]\n\\[ A_1^2 =\n    \\begin{bmatrix}\n    \\cos(\\theta_2) & -\\sin(\\theta_2) & 0 & a_2 \\cos(\\theta_2)\\\\\n    \\sin(\\theta_2) & \\cos(\\theta_2) & 0 & a_2\\sin(\\theta_2)\\\\\n    0 & 0 & 1 & d_0\\\\\n    0 & 0 & 0 & 1\n    \\end{bmatrix}\n, A_2^3 =\n    \\begin{bmatrix}\n    1 & 0 & 0 & 0\\\\\n    0 & 1 & 0 & 0\\\\\n    0 & 0 & 1 & -d_3\\\\\n    0 & 0 & 0 & 1\n    \\end{bmatrix}\n\\]\n\\[ A_3^4 =\n    \\begin{bmatrix}\n    \\cos(\\theta_4) & -\\sin(\\theta_4) & 0 & 0\\\\\n    \\sin(\\theta_4) & \\cos(\\theta_4) & 0 & 0\\\\\n    0 & 0 & 1 & 0\\\\\n    0 & 0 & 0 & 1\n    \\end{bmatrix}\n\\] \\[T_3^b = A_b^0 \\cdot A_0^1 \\cdot A_1^2 \\cdot A_2^3 \\cdot A_3^4\\]\nHence the pose of the end effector \\(\\mathbf{K(q)}\\) is given by \\[\\mathbf{K(q)} =\n        \\begin{bmatrix}\n        T_3^b(0,3) \\\\\n        T_3^b(1,3) \\\\\n        T_3^b(2,3) \\\\\n        \\theta_1 + \\theta_2 + \\theta_4\n        \\end{bmatrix}\n\\]\n\n\nFind the Jacobian of the Manipulator\nThe Jacobian which is then used to find the Inverse Jacobian of the system. Using the frames given we calculate the geometric Jacobian using the Equation.\n\n\n\nwhere \\[ p_0 = [0,0,0]^T \\] \\[ p_1 = A_0^1(0:2,4)\\] \\[ p_2= A_0^2(0:2,4) ,where \\: A_0^2 = A_1^2 \\cdot A_0^1\\] \\[ p_3= A_0^3(0:2,4) ,where \\: A_0^3 = A_2^3 \\cdot A_0^2\\] \\[ p= T_3^b(0:2,4) \\]\nand \\(Z_i\\) are the \\(Z\\) components of each of the frame \\(i\\) with respect to the base frame.\nWe find the Jacobian \\(J(q)\\) to be\n\n\n\nInverting it we get \\(J^{-1}(q)\\)\n\n\n\n\n\nResults Using Jacobian Inverse\nThe error vs time chart for the end effector positions along X, Y, Z and theta are given.\n\n\n\n\n\nUsing Jacobian Transpose\nThe inverse kinematics equation for the manipulator using Jacobian Transpose \\(J_a(q)^{T}\\) is given by \\[ \\dot{q} = J_a(q)^{T}Ke\\]\nThe tranpose of the jacobian is\n\n\n\nThis method of determining joint velocities is more computationally efficient that using the inverse. However, it has lower accuracy. Hence it is used for cases with larger error tolerance.\nThe equation given above is modeled using the following kinematic control schematic diagram.\n\n\n\n\n\nSimulink Block Diagram\n\n\n\nThe discrete integrator block is used to integrate to for every millisecond sample time. The Simout blocks are used to send the and X values to the workspace. The direct kinematics block remains the same as before.\n\n\nResults Using Jacobian Transpose\n\n\n\nFrom the results it is evident that the tracking error of the Jacobian Transpose control is 3 to 4 orders of magnitude higher than the Jacobian Inverse based Control but the runtime of the code is significantly shorter."
  },
  {
    "objectID": "posts/7_Manipulator_Kinematics/index.html#part-2",
    "href": "posts/7_Manipulator_Kinematics/index.html#part-2",
    "title": "SCARA Manipulator: Kinematic Control Part I",
    "section": "Part 2",
    "text": "Part 2\nSuppose we relax one component in the operational space, we can maximize the distance from the mechanical joint limits hence avoiding unstable regions and singularities.\nHere I implement an algorithm for kinematic inversion with Jacobian pseudo-inverse along the given trajectory maximizing in two separate cases the distance from the mechanical joint limits ( by relaxing the orientation component ϕ).\n\\[ \\dot{q} = J^+_a(\\dot{x}_d+Ke) + (I - J_a^+J_a)\\dot{q}_a\\]\nWhere \\(J^+_a\\) is the Jacobian PseudoInverse.\n\n\n\n\\(K\\) is a positive definite matrix and convergence depends on the eigen values finally the \\(q_a\\) enables us to generate internal motions and optimize for certain criteria without changing end effector position. In our case we want to stay as close as possible to the center of the joint range.\n\\[ \\dot{q_a} = k_0 \\left( \\frac{\\partial w(q)}{\\partial q} \\right)\\]\nwhere \\(k_0 &gt; 0\\) and \\(q_a\\) is a (secondary) objective function of the joint variables.\n\\[\nw(q) = \\frac{1}{2n} \\sum_{i=1}^n \\left( \\frac{ q_i - \\bar{q_i}}{q_{iM}-q_{im}} \\right)\\]\nHere \\(w(q)\\) is a cost function that when minimized ensures that the joint positions are close to the center of its range.\\(q_{iM}\\) and \\(q_{im}\\) denotes the maximum and minimum joint limits respectively and \\(\\bar{q_i}\\) is the middle value of the joint range; thus, by maximizing this distance, redundancy is exploited to keep the joint variables as close as possible to the centre of their ranges.\n\nSimulink Block Diagram\n\n\n\nThe Simulink Network above is very similar to the previous block diagrams. However it has a few key differences.\nFirstly, the orientation term has been relaxed. Hence \\(\\theta_d\\) and \\(\\dot{\\theta_d}\\) are not input into the system.The direct kinematics block is the same as that implemented in previous questions. However, I have used a second block to relax the orientation component. In essence, converting a 4X1 matrix to a 3X1 matrix.\nSecondly I have implemented two subsystems. Subsystem one takes input \\(q\\) and returns the Jacobian \\(J\\) and the jacobian pseudo inverse \\(J^+_a\\).\n\n\n\nSub system 2 takes \\(q\\), the Jacobian and the Jacobian pseudo inverse as input and returns the term \\((I - J_a^+J_a)\\dot{q}_a\\) .\n\n\n\n\n\nPart 2 Results\nThe error vs time chart for the end effector positions along X,Y and Z axes are given."
  },
  {
    "objectID": "posts/8_Manipulator_Obstacle_Avoidance/index.html",
    "href": "posts/8_Manipulator_Obstacle_Avoidance/index.html",
    "title": "SCARA Manipulator: Kinematic Control Part II",
    "section": "",
    "text": "This project will cover the following topics:  1. Second Order inverse differential kinematics of a Scara robotic Arm  2. Obstacle Avoidance by maximizing distance between joins and an Obstacle\nConsider the SCARA manipulator depicted below.\n\n\n\n\n\nOur Goal is to have the SCARA manipulator end effector follow the given position and velocity trajectories.\n\nImplement in Matlab/Simulink a second order algorithm for kinematic inversion with jacobian inverse along the given trajectory. Adopt the Euler integration rule with integration time 1 ms.\n\n\nOur Goal is to have the SCARA manipulator end effector follow the given position and velocity trajectories.\n\nImplement in Matlab/Simulink a second order algorithm for kinematic inversion with jacobian inverse along the given trajectory. Adopt the Euler integration rule with integration time 1 ms.\n\n\n\nThe Second order inverse kinematics equation for the given manipulator is given by\n\\[ \\ddot{q} = J^{-1}_a(q)(\\ddot{x}_d + K_d\\dot{e}+K_pe-\\dot{J_a}(q,\\dot{q})\\dot{q})\\]\nWhere \\(J^{-1}_a(q)\\) is the inverse of the Jacobian. \\(\\ddot{x_d}\\) is the acceleration trajectory. \\(e\\) is the error between the given trajectory and the end effector position. \\(K_d\\) and \\(K_p\\) are gains. For the SCARA robot the Jacobian is a 4×4 matrix. Since the \\(Z\\) component of the end effector is parallel to the \\(Z\\) component of the base frame the Geometric Jacobian is equal to the Analytical Jacobian.\nThe equation given above is modeled using the following kinematic control schematic diagram.\n\n\n\n\n\n\n\n\n\nThe time series trajectory data is imported from the workspace. A subtract block is used to calculate the error between the required trajectory and the actual position of the end effector which is calculated using direct kinematics. The error signal is passed through a gain block.\n\\(J_a(q,\\dot{q})\\) is calculated by differentiating \\(J_a(q)\\) with respect to \\(t\\).\n\\[\\frac{dJ_a}{dt} =\\frac{dJ_a}{dq}∙\\frac{dq}{dt}\\]\nWe get \\(J_a(q)\\) as\n\n\n\nFinally the function “q_double_dot” is used to bring together all the terms and calculate \\[\\ddot{q} = J^{-1}_a(q)(\\ddot{x_d}+K_d\\dot{e}+K_pe-\\dot{J_a}(q,\\dot{q})\\dot{q})\\]\nHere \\(J_a^{-1}(q)\\) is found to be\n\n\n\n\\(\\ddot{q}\\) is integrated twice to get \\(q\\). The mechanical joint limits are entered in the discrete integrator block . The upper saturation limit is \\([π/2, \\: π/4, \\:  1, \\:  2π]^T\\) and the lower saturation limit is \\([−π/2, \\: −π/2, \\: 1/4, \\: −2π]^T\\).\n\n\n\nJoint space trajectories are found to be.\n\n\n\nThe error vs time chart for the end effector positions along X, Y and Θ are ."
  },
  {
    "objectID": "posts/8_Manipulator_Obstacle_Avoidance/index.html#part-2",
    "href": "posts/8_Manipulator_Obstacle_Avoidance/index.html#part-2",
    "title": "SCARA Manipulator: Kinematic Control Part II",
    "section": "Part 2",
    "text": "Part 2\nSuppose to relax one component in the operational space (relax the z component), implement in Matlab/Simulink the second order algorithm for kinematic inversion with Jacobian pseudo-inverse along the given trajectory maximizing the distance from an obstacle along the path. Suppose that the obstacle is a sphere centered in p = [0.4 −0.7 0.5]⊤ with radius 0.2 m.\n\nExploit Redundancy using Jacobian Pseudo-inverse\n\\[ \\ddot{q} = J^{+}_a(\\ddot{x}_d + K_d\\dot{e}+K_pe-\\dot{J_a}(q,\\dot{q})\\dot{q})+(I_n-J^{+}_aJ_a)\\ddot{q}_0 \\]\nWhere \\(J^{+}_a\\) is the Jacobian Pseudo inverse.\n\n\n\n\\(\\dot{q}_0\\) enables us to generate internal motions and optimize for certain criteria without changing end effector position. In our case we want to stay as close as possible to the center of the joint range.\n\\[\\dot{q}_0 = k_0 \\left( \\frac{\\partial w(q)}{\\partial q} \\right)^T\\]\nwhere \\(k_0 &gt; 0\\) and \\(w(q)\\) is a (secondary) objective function of the joint variables.\n\\[w(q)=   _{p, o}^{min} ||p(q)−o||\\]\nHere \\(w(q)\\) is a cost function that when minimized ensures that the end distance between the effector position pq and the obstacle \\((o)\\) is maximized.\nThe distance is calculated using the equation\n\\[w(q) = \\sqrt{(p_x−o_x)^2+(p_y−o_y)^2+(p_z−o_z)^2}\\]\nSubstituting \\(p_x, p_y\\) and \\(p_z\\) \\[ \\sqrt{\\left( \\frac{cos(\\theta_1+\\theta_2)}{2} + \\frac{cos(\\theta_1)}{2}-\\frac{2}{5} \\right)^2  + \\left( \\frac{sin(\\theta_1+\\theta_2)}{2} + \\frac{sin(\\theta_1)}{2}-\\frac{7}{10} \\right)^2 +  \\left( d_3 - \\frac{1}{2} \\right) }\\]\nIntegrating twice with respect to “q” and multiply gain ‘\\(k_0\\)’ we get \\(q_0\\).\n\n\nSimulink Block Diagram\n\n\n\nThe Simulink Network above is very similar to the previous block diagrams. However it has a few key differences.\nFirstly, the ‘\\(Z\\)’ term has been relaxed. Hence it is not input into the system. The direct_kin, xe_dot and J_dot_r*q_dot block is the same as that implemented in previous model. However,the code has been altered to accommodate the relaxed Jacobian. In essence, converting a 4×1 matrix to a 3×1 matrix.\nSecondly I have implemented the subsystem “[In - J_pinv.J]*q_dd”. Subsystem one takes input ‘q’ and returns the jacobian pseudo inverse ‘J_pinv’ and an additional term In− JA†JAq0.\n\n\n\n“[In - J_pinv.J]*q_dd” Subsystem\nThe subsystem is composed of two functions. The “J_pinv” function takes as input “q” and returns the Jacobian “J” and the Jacobian pseudo-inverse “J_pinv”. The function “calc_term” returns the term \\((I_n− J_a^†J_a)\\ddot{q_0}\\)\n\n\nResults\nJoint space trajectories are found to be. Notice that q3 is constant this correlates with the lowest position of the end effector. In essence, the maximum distance from the obstacle.\nThe error vs time chart for the end effector positions along X,Y and Θ axes are given."
  },
  {
    "objectID": "posts/8_Manipulator_Obstacle_Avoidance/index.html#overview",
    "href": "posts/8_Manipulator_Obstacle_Avoidance/index.html#overview",
    "title": "SCARA Manipulator: Kinematic Control Part II",
    "section": "",
    "text": "This project will cover the following topics:  1. Second Order inverse differential kinematics of a Scara robotic Arm  2. Obstacle Avoidance by maximizing distance between joins and an Obstacle\nConsider the SCARA manipulator depicted below.\n\n\n\n\n\nOur Goal is to have the SCARA manipulator end effector follow the given position and velocity trajectories.\n\nImplement in Matlab/Simulink a second order algorithm for kinematic inversion with jacobian inverse along the given trajectory. Adopt the Euler integration rule with integration time 1 ms.\n\n\nOur Goal is to have the SCARA manipulator end effector follow the given position and velocity trajectories.\n\nImplement in Matlab/Simulink a second order algorithm for kinematic inversion with jacobian inverse along the given trajectory. Adopt the Euler integration rule with integration time 1 ms.\n\n\n\nThe Second order inverse kinematics equation for the given manipulator is given by\n\\[ \\ddot{q} = J^{-1}_a(q)(\\ddot{x}_d + K_d\\dot{e}+K_pe-\\dot{J_a}(q,\\dot{q})\\dot{q})\\]\nWhere \\(J^{-1}_a(q)\\) is the inverse of the Jacobian. \\(\\ddot{x_d}\\) is the acceleration trajectory. \\(e\\) is the error between the given trajectory and the end effector position. \\(K_d\\) and \\(K_p\\) are gains. For the SCARA robot the Jacobian is a 4×4 matrix. Since the \\(Z\\) component of the end effector is parallel to the \\(Z\\) component of the base frame the Geometric Jacobian is equal to the Analytical Jacobian.\nThe equation given above is modeled using the following kinematic control schematic diagram.\n\n\n\n\n\n\n\n\n\nThe time series trajectory data is imported from the workspace. A subtract block is used to calculate the error between the required trajectory and the actual position of the end effector which is calculated using direct kinematics. The error signal is passed through a gain block.\n\\(J_a(q,\\dot{q})\\) is calculated by differentiating \\(J_a(q)\\) with respect to \\(t\\).\n\\[\\frac{dJ_a}{dt} =\\frac{dJ_a}{dq}∙\\frac{dq}{dt}\\]\nWe get \\(J_a(q)\\) as\n\n\n\nFinally the function “q_double_dot” is used to bring together all the terms and calculate \\[\\ddot{q} = J^{-1}_a(q)(\\ddot{x_d}+K_d\\dot{e}+K_pe-\\dot{J_a}(q,\\dot{q})\\dot{q})\\]\nHere \\(J_a^{-1}(q)\\) is found to be\n\n\n\n\\(\\ddot{q}\\) is integrated twice to get \\(q\\). The mechanical joint limits are entered in the discrete integrator block . The upper saturation limit is \\([π/2, \\: π/4, \\:  1, \\:  2π]^T\\) and the lower saturation limit is \\([−π/2, \\: −π/2, \\: 1/4, \\: −2π]^T\\).\n\n\n\nJoint space trajectories are found to be.\n\n\n\nThe error vs time chart for the end effector positions along X, Y and Θ are ."
  },
  {
    "objectID": "posts/6_RLAN_3_UKF/index.html",
    "href": "posts/6_RLAN_3_UKF/index.html",
    "title": "Unscented Kalman Filter",
    "section": "",
    "text": "In this project, I develop an Unscented Kalman Filter (UKF) to fuse the inertial data already used in project 1 and the vision-based pose and velocity estimation developed in project 2."
  },
  {
    "objectID": "posts/6_RLAN_3_UKF/index.html#unscented-kalman-filter-a-deep-dive",
    "href": "posts/6_RLAN_3_UKF/index.html#unscented-kalman-filter-a-deep-dive",
    "title": "Unscented Kalman Filter",
    "section": "Unscented Kalman Filter: A Deep Dive",
    "text": "Unscented Kalman Filter: A Deep Dive\nThe Unscented Kalman Filter is an advanced state estimation algorithm that addresses the limitations of the traditional Kalman Filter by effectively handling nonlinear systems. The UKF operates on the principle of capturing the mean and covariance of a probability distribution through a deterministic sampling technique known as the Unscented Transform.\n\nThe Unscented Transform\nThe Unscented Transform selects a set of points (sigma points) around the mean state in such a way that their mean and covariance match the original distribution. These points are then propagated through the nonlinear system model, allowing for an accurate approximation of the resulting mean and covariance without the need for linearization.\n\n\nMathematical Formulation\nThe UKF update cycle can be broken down into the following steps:\n\nSelection of Sigma Points: Sigma points are chosen to capture the mean and covariance of the state estimate. This selection is critical for accurately representing the state’s uncertainty.\nPrediction Step: The sigma points are propagated through the system’s dynamics model to predict the state at the next time step. This step involves calculating the predicted state mean and covariance, incorporating the process noise.\nUpdate Step: Upon receiving a new measurement, the sigma points are updated to reflect the new information. This involves computing the Kalman gain, updating the state estimate, and adjusting the covariance to reduce uncertainty based on the measurement noise.\n\nThe equations for these steps are intricate, involving matrix operations to ensure that the predicted state and covariance accurately reflect the system’s dynamics and the measurements’ influence."
  },
  {
    "objectID": "posts/6_RLAN_3_UKF/index.html#part-i",
    "href": "posts/6_RLAN_3_UKF/index.html#part-i",
    "title": "Unscented Kalman Filter",
    "section": "Part I",
    "text": "Part I\nThe objective is to utilize the information from project 2 to approximate the status of the robot drone. The robot’s condition is made up of the ensuing variables: \\[ [x, y, z, roll, pitch, yaw, \\dot{x}, \\dot{y}, \\dot{z}, g_x, g_y, g_z, a_x, a_y, a_z]^T \\] Additionally, \\(g_x, g_y\\), and \\(g_z\\) denote the biases of the gyroscope in the \\(x, y\\), and \\(z\\) axes of the robot’s body frame, respectively, while \\(a_x, a_y\\), and \\(a_z\\) indicate the biases of the accelerometer in the \\(x, y,\\) and $ z$ axes of the robot’s body frame. To illustrate the drone’s rotations, we will utilize the ZYX convention of Euler angle rotation.\n\nResults\nOur implementation showed promising results, closely aligning with the actual state measured by the Motion Capture (Mocap) Vicon System. Below are some visual representations of our findings:\n\n Part1 Dataset1 State Estimation Result\n\n\n Part1 Dataset2 State Estimation Result\n\nThe results show the estimated state versus the actual state for Dataset 1 and 2, illustrating the UKF’s accuracy in tracking the drone’s position and orientation.\nThese images provide a visual understanding of the UKF’s performance and the impact of relying on different sets of measurements for state estimation."
  },
  {
    "objectID": "posts/6_RLAN_3_UKF/index.html#part-2",
    "href": "posts/6_RLAN_3_UKF/index.html#part-2",
    "title": "Unscented Kalman Filter",
    "section": "Part 2",
    "text": "Part 2\nThe problem at hand involves using the Unscented Kalman Filter to predict the position, velocity, and orientation of a drone, with the help of only velocity measurements from the Vicon sensor and control inputs from the IMU. This means that the predictions for position and velocity are expected to be quite accurate, but the orientation in the z axis may drift due to the lack of Vicon measurements. The prediction step remains the same as in Part 1, but the observation model in the update step needs to be modified since the linear velocity data is in the camera frame and needs to be converted to the world frame. \\[Z_t = g(x_2, x_3, w^c_w) + v = R^C_B*R^B_W x_3 - R^C_BS(r^B_{BC}) R^B_C * w^C_W + v\n\\] Where: \\(S\\) is the skew symmetric matrix of the vector that points from the camera to the body(IMU) expressed in the body frame and v is the noise.In the context of implementing the Unscented Kalman Filter, the problem is that the observation model is nonlinear. To approximate the Gaussian probability distribution of the output of the model, we need to perform the Unscented Transform, as we did in Part 1. However, unlike Part 1, the noises are not part of the dynamic equations and are just added at the end. Therefore, we do not need to augment the state vector. The mean of the output of the observation model and the covariance are calculated as in Part 1. The cross-covariance between input and output is also calculated using the cross-covariance matrix.\n\\[{\\mu_{k}=\\sum_{i=0}^{2\\pi}W_{i}^{(m)}\\hat{\\mathcal{Y}}_{k}^{(i)},}\\]\n\\[{S_{k}=\\sum_{i=0}^{2\\pi}W_{i}^{(c)}(\\hat{\\mathcal{\\mathcal{Y}}}_{k}^{(i)}-\\mu_{k})(\\hat{\\mathcal{\\mathcal{Y}}_{k}}^{(i)}-\\mu_{k})^{T}+R_{k},}\\]\n\\[{C_{k}=\\sum_{i=0}^{2\\pi}W_{i}^{(c)}(\\mathcal{X}_{k}^{-(i)}-m_{k}^{-})(\\hat{\\mathcal{Y}}_{k}^{(i)}-\\mu_{k})^{\\top}.}\\]\nWe then update the mean and covariance using the following equations \\[K_k = C_k S_k^{-1} \\] \\[m_k = \\bar{m}_{k} + K_k [y_k - \\mu_k]\\] \\[p_k = \\bar{p}_{k}- K_k S_k K_k^{\\top}\\]"
  },
  {
    "objectID": "posts/6_RLAN_3_UKF/index.html#results-1",
    "href": "posts/6_RLAN_3_UKF/index.html#results-1",
    "title": "Unscented Kalman Filter",
    "section": "Results",
    "text": "Results\n\n Part 2 Dataset 1 State Estimation Results\n\n\n Part 2 Dataset 2 State Estimation Results\n\nThe results demonstrates the drift encountered when relying solely on velocity measurements, highlighting the challenges of maintaining accurate orientation estimation without direct positional data.\nThe project underscored the effectiveness of the Unscented Kalman Filter in handling nonlinearities and uncertainties inherent in real-world robotic localization and navigation tasks. Through this hands-on experience, we gained deeper insights into the challenges and complexities of state estimation, paving the way for future advancements in robotic autonomy."
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html#overview",
    "href": "posts/5_RLAN_2_VO/index.html#overview",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "",
    "text": "This project is composed of 2 parts. In part one we have to determine the position and orientation of a quadrotor flying over a Mat of April Tags. For the second part, I estimate the linear and angular velocity of the drone using Optical Flow.\nThe data for this project was collected using a Nano+ quadrotor that was either held by hand or flown through a prescribed trajectory over a mat of AprilTags, each of which has a unique ID.Thes images captured by the quadrotor are in a time sequence. We know the coordinates of each point on the April Tag mat. Using this information we can find the position and orientation of the quadrotor in the world frame.\nThe intrinsic camera calibration matrix and the transformation between the camera and the robot center are known. These Two photos are included to visualize the camera-robot body transform\n\n\n\n\n\n\n\n\n\n\n\n(a) Side View\n\n\n\n\n\n\n\n\n\n\n\n(b) Top View\n\n\n\n\n\n\n\nFigure 1: Camera to Body transformation\n\n\n\nThe data contains a struct array of image data called data, which holds all of the data necessary to do pose estimation. This includes the time stamps, April Tag IDs observed for each time stamp and the location of the April Tag corners and centers in image coordinates.\nThis is a rectified image of the April Tag Mat.\n\n\n\nApril Tag Mat"
  },
  {
    "objectID": "posts/7_Manipulator_Kinematics/index.html#overview",
    "href": "posts/7_Manipulator_Kinematics/index.html#overview",
    "title": "SCARA Manipulator: Kinematic Control Part I",
    "section": "",
    "text": "This project will cover the following topics:  1. Deriving Forward kinematics using DH Parmamertization of a robotic Arm  2. Forward Differential Kinematics  3. Inverse Differential Kinematics (using Jacobian Inverse and Jacobian Transpose) 4. Exploiting redundant DOFs to add a secondary objective\nConsider the SCARA manipulator depicted below.\n\n\n\n\n\nOur Goal is to have the SCARA manipulator end effector follow the given position and velocity trajectories.\n\nThe manipulator parameteres are \\[ d_0 = 1 \\: m\\] \\[a_1 = a_2 = 0.5 \\: m\\] \\[ \\theta_{1_{min}} = - \\pi / 2 \\: rad, \\theta_{1_{max}} = \\pi / 2 \\: rad \\] \\[ \\theta_{2_{min}} = - \\pi / 2 \\: rad, \\theta_{2_{max}} =  \\pi / 4 \\: rad  \\] \\[ d_{3_{min}} = 0.25\\: m , d_{3_{max}} = 1\\: m \\] \\[ \\theta_{4_{min}} = - 2\\pi \\: rad, \\theta_{4_{max}} =  2\\pi \\: rad  \\]\nThe frames are depicted into the figure and the DH parameters are:\n\nDH Parameters\n\n\n\n\n\n\n\n\n\n\n\\(d_i\\)\n\\(\\alpha_i\\)\n\\(\\theta_i\\)\n\\(a_i\\)\n\n\n\n\nLink 1\n\\(0\\)\n\\(0\\)\n\\(\\theta_1\\)\n\\(a_1\\)\n\n\nLink 2\n\\(0\\)\n\\(0\\)\n\\(\\theta_2\\)\n\\(a_2\\)\n\n\nLink 3\n\\(d_3\\)\n\\(0\\)\n\\(0\\)\n\\(0\\)\n\n\nLink 4\n\\(0\\)\n\\(0\\)\n\\(\\theta_4\\)\n\\(0\\)\n\n\n\nwhere \\(d_i\\) and \\(\\alpha\\) are the translational and rotational offsets along the \\(x\\) axis of frame \\(i-1\\) and frame \\(i\\). \\(\\theta_i\\) and \\(a_i\\) are the joint angles and link lengths respectively.\nNote that the 0 frame is not coincident with the b frame. There is a translation from the ground plane denoted with \\(d_0 = 1\\). The frame 4 is coincident with the frame 3 at the starting. Be careful on the \\(d3\\) component. The range of values is always positive. When the arm is fully extended (down towards the floor) the value is 1m whereas 0.25 when retracted (away from the floor). However, when you build your matrix note that \\(d_3\\) moves along \\(−z_2\\) axis and for this reason you translation in \\(A_{23}\\) should be negative as \\(−d_3\\)."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/9_L1_adaptive_control/index.html",
    "href": "posts/9_L1_adaptive_control/index.html",
    "title": "L1 Adaptive Control",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/3_L1_Adaptive_Control/index.html",
    "href": "posts/3_L1_Adaptive_Control/index.html",
    "title": "L1 Augmented Geometric Controller",
    "section": "",
    "text": "Aerial robots are required to remain operational even in the event of system disturbances, damages, or failures to ensure resilient and robust task completion and safety. One common failure case is propeller damage, which presents a significant challenge in both quantification and compensation. In this letter, we propose a novel adaptive control scheme capable of detecting and compensating for multi-rotor propeller damages, ensuring safe and robust flight performances. Our solution combines an L1 adaptive controller with an optimization routine for damage inference and compensation of single or dual propellers, with the capability to seamlessly transition to a fault-tolerant solution in case the damage becomes severe. We experimentally identify the conditions under which the L1 adaptive solution remains preferable over a fault tolerant alternative. Experimental results validate the proposed approach demonstrating the ability of our solution to adapt and compensate onboard in real time on a quadrotor for damages even when multiple propellers are damaged."
  },
  {
    "objectID": "posts/line_follower/index.html#background",
    "href": "posts/line_follower/index.html#background",
    "title": "Race Circuit Line Follower",
    "section": "Background:",
    "text": "Background:\nThe goal of this project was to perform a PID reactive control capable of following the line painted on a racing circuit. PID control is one of the fundamental concepts of Linear control systems. The Controller continuously calculates an error value as the difference between desired output and the current output and applies a correction based on proportional, integral and derivative terms(denoted by P, I, D respectively). The control signal u[k] for a PID controller can be expressed as follows. \n\nProportional:\nProportional Controller gives an output which is proportional to the current error. The error e[k] is multiplied with a proportional gain(Kp) to get the output. And hence, is 0 if the error is 0.In this case the error is the difference between the center of the image and the centroid of the racing line.\n\n\nIntegral:\nIntegral Controller provides a necessary action to eliminate the offset error which is accumulated by the P Controller.It integrates the error over a period of time until the error value reaches to zero.\n\n\nDerivative:\nDerivative Controller gives an output depending upon the rate of change or error with respect to time. It gives the kick start for the output thereby increasing system response.\nThe integral and dervivative errors are calculated as follows."
  },
  {
    "objectID": "posts/line_follower/index.html#tuning-the-pid-controller.",
    "href": "posts/line_follower/index.html#tuning-the-pid-controller.",
    "title": "Race Circuit Line Follower",
    "section": "Tuning the PID controller.",
    "text": "Tuning the PID controller.\nFirstly, we must setup the P controller as per Ziegler Nichols method. Keep adjusting the value of the constant, till we get a value where there occurs it has neither unstable oscillations and nor slow response.   \nOnce you get oscillations of constant amplitude you can adjust the derivative gains (Kd). After this the vehicle was much more stable and tracked the line accurately. Finally I modified the Integral gain which I found to have minimal effect on the system. But nonetheless helped to avoid any steady state errors."
  },
  {
    "objectID": "posts/line_follower/index.html#results-and-code",
    "href": "posts/line_follower/index.html#results-and-code",
    "title": "Race Circuit Line Follower",
    "section": "Results and Code",
    "text": "Results and Code\n\nFinal Code"
  },
  {
    "objectID": "posts/line_follower/index.html#learning",
    "href": "posts/line_follower/index.html#learning",
    "title": "Race Circuit Line Follower",
    "section": "Learning",
    "text": "Learning\nOne mistake I made was to set the velocity of the car too high. Since the cycle time of the system is only 12Hz the controller could not detect the change in error fast enough. This led to understeer and multiple head on collision with the track walls. It’s always a good idea to start with the minimal speed requirement when programming a controller and increase it once the system is more robust."
  },
  {
    "objectID": "posts/line_follower/index.html#references",
    "href": "posts/line_follower/index.html#references",
    "title": "Race Circuit Line Follower",
    "section": "References:",
    "text": "References:\nJde Robotics Visual Line Follow What is PID control? MATLAB Discrete time equations for PID control"
  },
  {
    "objectID": "posts/9_SLAM_Maze/index.html",
    "href": "posts/9_SLAM_Maze/index.html",
    "title": "Virtual Maze Exploration and Navigation",
    "section": "",
    "text": "For this project, the task was to explore a maze of images, mapping the environment as we navigated. Once the map was complete, we were given four target images, and the goal was to locate and navigate to these images as quickly as possible.\nThis was a collaborative effort, where I experimented with various SLAM algorithms, such as ORB SLAM (2&3) and Stella V SLAM. Unfortunately, these methods did not yield accurate results for multiple reasons. First, we had to overcome the Sim-to-Real gap, as SLAM techniques developed for real-world datasets struggled with the simulated environment. Estimating depth posed a major challenge, as implementing depth estimation techniques led to system lag and poor accuracy. Additionally, these algorithms relied on inertial data from IMU sensors for state estimation, but we only had access to visual data in the simulation. While these methods handled linear movement effectively, rotational movements caused severe drift and localization loss.\nWe also tried computationally intensive approaches, such as visual reconstruction using COLMAP, which provided accurate results for smaller maps. However, for larger maps, the increased search space made these methods infeasible to run on our laptop hardware, and since speed was the main performance criterion, we couldn’t afford such solutions.\nUltimately, we developed a custom lightweight approach using Visual Odometry. I implemented a method that utilized homogeneous transformations and the essential matrix to extract the pose (R, t) between frames. We experimented with different features (SIFT, ORB) and matching algorithms, eventually finding that SuperPoint and SuperGlue provided the best tracking performance. Although computationally heavier than traditional methods, the accuracy justified the trade-off.\nWe also explored visual place recognition techniques like VLAD, NetVLAD, and Bag of Visual Words, settling on VLAD for its simplicity and effectiveness in our specific use case.\nOnce the exploration phase was complete, our bot followed the previously mapped path to each target location. This approach allowed us to finish with the fastest time in the competition, earning recognition from Professor Chen Feng.\nLinkedIn Post"
  }
]