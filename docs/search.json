[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projects",
    "section": "",
    "text": "Vision Based Pose and Velocity Estimation\n\n\n\n\n\n\n\nComputer Vision\n\n\nVisual Odometry\n\n\nMATLAB\n\n\nQuadrotors\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2023\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n  \n\n\n\n\nState Estimation with Extended Kalman Filter\n\n\n\n\n\n\n\nState Estimation\n\n\nFilters\n\n\nMATLAB\n\n\nQuadrotors\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2023\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n  \n\n\n\n\nRace Circuit Line Follower\n\n\n\n\n\n\n\nControl Systems\n\n\nopencv\n\n\npython\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\nSuraj Kiron Nair\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2022\n\n\nSuraj K Nair\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first blog post. Welcome!\n\nTo me robotics is a fascinating discipline that combines engineering, computer science, and artificial intelligence. ie its the coolest thing ever. As a member of the Agile Robotics and Perception Lab(ARPL) at NYU I have the opportunity to work with this cutting edge technology.\nI will be using this blog to post about projects I’m working on or even random things I find interesting. I have a lot of plans in store for this summer, so I am writing this blog as a record of everything I’m doing. I’m halfway through my Masters in Mechatronics and Robotics and this blog has been a long time coming.\nI believe that robotics has the power to revolutionize the way we live, work, and interact with the world around us. Through this blog, I wish to share my passion for robotics and record my journey through this vast field.\nSo, buckle up and get ready to embark on a thrilling journey into the realm of robotics. Let’s explore the limitless possibilities and discover the future that robots hold for us all."
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#background",
    "href": "posts/1_Line_ Follower/index.html#background",
    "title": "Race Circuit Line Follower",
    "section": "Background:",
    "text": "Background:\nThe goal of this project was to perform a PID reactive control capable of following the line painted on a racing circuit. PID control is one of the fundamental concepts of Linear control systems. The Controller continuously calculates an error value as the difference between desired output and the current output and applies a correction based on proportional, integral and derivative terms(denoted by P, I, D respectively). The control signal u[k] for a PID controller can be expressed as follows. \n\nProportional:\nProportional Controller gives an output which is proportional to the current error. The error e[k] is multiplied with a proportional gain(Kp) to get the output. And hence, is 0 if the error is 0.In this case the error is the difference between the center of the image and the centroid of the racing line.\n\n\nIntegral:\nIntegral Controller provides a necessary action to eliminate the offset error which is accumulated by the P Controller.It integrates the error over a period of time until the error value reaches to zero.\n\n\nDerivative:\nDerivative Controller gives an output depending upon the rate of change or error with respect to time. It gives the kick start for the output thereby increasing system response.\nThe integral and dervivative errors are calculated as follows."
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#tuning-the-pid-controller.",
    "href": "posts/1_Line_ Follower/index.html#tuning-the-pid-controller.",
    "title": "Race Circuit Line Follower",
    "section": "Tuning the PID controller.",
    "text": "Tuning the PID controller.\nFirstly, we must setup the P controller as per Ziegler Nichols method. Keep adjusting the value of the constant, till we get a value where there occurs it has neither unstable oscillations and nor slow response.   \nOnce you get oscillations of constant amplitude you can adjust the derivative gains (Kd). After this the vehicle was much more stable and tracked the line accurately. Finally I modified the Integral gain which I found to have minimal effect on the system. But nonetheless helped to avoid any steady state errors."
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#results-and-code",
    "href": "posts/1_Line_ Follower/index.html#results-and-code",
    "title": "Race Circuit Line Follower",
    "section": "Results and Code",
    "text": "Results and Code\n\nFinal Code"
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#learning",
    "href": "posts/1_Line_ Follower/index.html#learning",
    "title": "Race Circuit Line Follower",
    "section": "Learning",
    "text": "Learning\nOne mistake I made was to set the velocity of the car too high. Since the cycle time of the system is only 12Hz the controller could not detect the change in error fast enough. This led to understeer and multiple head on collision with the track walls. It’s always a good idea to start with the minimal speed requirement when programming a controller and increase it once the system is more robust."
  },
  {
    "objectID": "posts/1_Line_ Follower/index.html#references",
    "href": "posts/1_Line_ Follower/index.html#references",
    "title": "Race Circuit Line Follower",
    "section": "References:",
    "text": "References:\nJde Robotics Visual Line Follow What is PID control? MATLAB Discrete time equations for PID control"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a student pursuing my Masters in Mechatronics and Robotics at NYU Tandon.\n\n\nNYU Tandon Institute of Technology | New York  MS Mechatronics and Robotics | Sept 2022 - May 2024\nRamaiah Institute of Technology | Bengaluru | India  BE Mechanical Engineering | Sept 2015 - June 2021\n\n\n\nARPL | Graduate Engineer | Jan 2023 - present\nICER IISc | Research Assistant | June 2021 - May 2022"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About",
    "section": "",
    "text": "NYU Tandon Institute of Technology | New York  MS Mechatronics and Robotics | Sept 2022 - May 2024\nRamaiah Institute of Technology | Bengaluru | India  BE Mechanical Engineering | Sept 2015 - June 2021"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "About",
    "section": "",
    "text": "ARPL | Graduate Engineer | Jan 2023 - present\nICER IISc | Research Assistant | June 2021 - May 2022"
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html",
    "href": "posts/4_RLAN_1_EKF/index.html",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "",
    "text": "In this project I use a Extended Kalman Filter to Localize a quadrotor. I use the body frame acceleration and angular velocity from the onboard IMU as your control inputs. The measurement will be given by the pose or velocity from the Vicon. Vicon data is in the following format: \\[ [x, y, z, roll, pitch, yaw, v_x, v_y, v_z, \\omega_x, \\omega_y, \\omega_z]^T \\]\nThe on board processor of the robot collects synchronized camera and IMU data and sends them to the mission computer. At this stage, the camera data should not be used. The sensor data is decoded into standard MATLAB format. Note that since the sensor data is transmitted via wireless network, there may or may not be a sensor packet available during a specific iteration of the control loop. A sensor packet is a struct that contains following fields:\nThe goal is to use an Extended Kalman Filter (EKF) to estimate the position, velocity, and orientation, and sensor biases of an Micro Aerial Vehicle. The Vicon velocity is given in the world frame, whereas the angular rate in the body frame of the robot. Furthermore, I use the body frame acceleration and angular velocity from the on board IMU as the inputs.\nI have implemented 2 versions of the filter. In the first one, the measurement update is given by the position and orientation from vicon, in the second one I use only the velocity from the Vicon.. In both parts, the process model is the same."
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#assumptions",
    "href": "posts/4_RLAN_1_EKF/index.html#assumptions",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "Assumptions",
    "text": "Assumptions\nWe make the assumption that the noise in the readings obtained from the IMU and Vicon adhere to a normal distribution. Additionally, we can assume the state derivative to be both continuous and differentiable, allowing us to linearize it. Based on these assumptions, we can utilize the EKF algorithm for predicting the state."
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#the-process-model",
    "href": "posts/4_RLAN_1_EKF/index.html#the-process-model",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "The Process Model: ",
    "text": "The Process Model: \nThe state \\(\\mathbf{X}\\) is given by \\[\\mathbf{X} =\n\\begin{bmatrix}\n\\mathbf{x_1} \\\\\n\\mathbf{x_2} \\\\\n\\mathbf{x_3} \\\\\n\\mathbf{x_4} \\\\\n\\mathbf{x_5}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{p} \\\\\n\\mathbf{q} \\\\\n\\mathbf{\\dot{p}} \\\\\n\\mathbf{b_g} \\\\\n\\mathbf{b_a}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\mathbf{position} \\\\\n\\mathbf{orientation} \\\\\n\\mathbf{linear \\, velocity} \\\\\n\\mathbf{gyroscope \\, bias} \\\\\n\\mathbf{accelerometer \\, bias}\n\\end{bmatrix}\n\\]\nDifferentiating we get the state transition equation as \\[\n\\mathbf{\\dot{X}} =\n\\begin{bmatrix}\n\\mathbf{x_3} \\\\\nG(\\mathbf{x_2})^{-1} (\\mathbf{\\omega_m} - \\mathbf{x_4} - \\mathbf{n_g}) \\\\\n\\mathbf{g}+R(\\mathbf{x_2})(\\mathbf{a_m}-\\mathbf{x_5})-\\mathbf{n_a} \\\\\n\\mathbf{n_{bg}} \\\\\n\\mathbf{n_{ba}}\n\\end{bmatrix}\n=\nf(\\mathbf{X}, \\mathbf{u}, \\mathbf{n})\n\\] Where \\(G(\\mathbf{x_2})^{-1}\\) maps orientation to angular velocity and is given by \\[\nG(\\mathbf{x_2})^{-1} =\n\\begin{bmatrix}\n\\frac{\\cos(z)\\sin(y)}{\\cos(y)} & \\frac{\\sin(z)\\sin(y)}{\\cos(y)} & 1\\\\\n-\\sin(z) & \\cos(z) & 0\\\\\n\\frac{\\cos(z)}{\\cos(y)} & \\frac{\\sin(z)}{\\cos(y)} & 0\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#prediction-step",
    "href": "posts/4_RLAN_1_EKF/index.html#prediction-step",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "Prediction Step:",
    "text": "Prediction Step:\n\n\n\nPrediction Equations\n\n\nWe use MATLAB symbolic library to determine \\(A_t\\) and \\(U_t\\). \\(Q_d\\) is the covariance of the IMU noise. In the prediction stage, the filter uses the system model to make a prediction of the next state of the system based on the current state and any control inputs from the IMU"
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#update-step",
    "href": "posts/4_RLAN_1_EKF/index.html#update-step",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "Update Step:",
    "text": "Update Step:\nThe prediction is used to estimate the measurement that will be obtained at the next time step. During the correction stage, the filter incorporates both the predicted measurement and the actual measurement taken at the following time step to refine its assessment of the system’s state. The refined estimate is a weighted sum of the predicted state and the actual measurement, with the filter’s assessment of the uncertainty in both the model and the measurements used to determine the weighting of each component using the Kalman Gain.\n\n\n\nUpdate Equations\n\n\nHere the observation model \\(\\mathbf{z}\\) is given by \\[\n\\mathbf{z}=\n\\begin{bmatrix}\n\\mathbf{p} \\\\\n\\mathbf{q} \\\\\n\\mathbf{r}\n\\end{bmatrix}\n+\n\\mathbf{v}\n=\n\\mathbf{C_t X}+\\mathbf{v}\n\\]\nWhere \\(C_t\\) is a selection matrix. For case 1 we select position and orientation \\[\nC_t =\n\\begin{bmatrix}\nI & 0 & 0 & 0 & 0 & 0\\\\\n0 & I & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\]\nFor case 2 we select Linear velocitiy \\[\nC_t =\n\\begin{bmatrix}\n0 & 0 & I & 0 & 0 & 0\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "posts/4_RLAN_1_EKF/index.html#results",
    "href": "posts/4_RLAN_1_EKF/index.html#results",
    "title": "State Estimation with Extended Kalman Filter",
    "section": "Results",
    "text": "Results\nI have compared both implementations of the Extended Kalman filter on 3 datasets. 3 being the most agressive flight.\n\nCase 1 Dataset 1\n\n\n\nState Estimation Dataset 1\n\n\n\n\nCase 1 Dataset 2\n\n\n\nState Estimation Dataset 2\n\n\n\n\nCase 1 Dataset 3\n\n\n\nState Estimation Dataset 3\n\n\n\n\nCase 2 Dataset 1\n\n\n\nState Estimation Dataset 1\n\n\n\n\nCase 2 Dataset 2\n\n\n\nState Estimation Dataset 2: shows yaw drift\n\n\n\n\nCase 2 Dataset 3\n\n\n\nState Estimation Dataset 3: shows yaw drift\n\n\nThe results show that the yaw estimate of the quadrotor drifts if we use only velocity measurements to update the EKF. Measuring pose gives us better tracking performance.\nNext Project: Vision Based Pose and Velocity Estimation"
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html",
    "href": "posts/5_RLAN_2_VO/index.html",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "",
    "text": "This project is composed of 2 parts. In part one we have to determine the position and orientation of a quadrotor flying over a Mat of April Tags. For the second part, I estimate the linear and angular velocity of the drone using Optical Flow.\nThe data for this project was collected using a Nano+ quadrotor that was either held by hand or flown through a prescribed trajectory over a mat of AprilTags, each of which has a unique ID.Thes images captured by the quadrotor are in a time sequence. We know the coordinates of each point on the April Tag mat. Using this information we can find the position and orientation of the quadrotor in the world frame.\nThe intrinsic camera calibration matrix and the transformation between the camera and the robot center are known. These Two photos are included to visualize the camera-robot body transform\n\n\n\n\n\n\n\n(a) Side View\n\n\n\n\n\n\n\n(b) Top View\n\n\n\n\nFigure 1: Camera to Body transformation\n\n\nThe data contains a struct array of image data called data, which holds all of the data necessary to do pose estimation. This includes the time stamps, April Tag IDs observed for each time stamp and the location of the April Tag corners and centers in image coordinates.\nThis is a rectified image of the April Tag Mat.\n\n\n\nApril Tag Mat"
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html#part-i-pose-estimation",
    "href": "posts/5_RLAN_2_VO/index.html#part-i-pose-estimation",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "Part I: Pose Estimation",
    "text": "Part I: Pose Estimation\n\nMethodology\nThe goal of this section is to use this data to determine the pose of the quadrotor. I have to use the points of the corners of each AprilTag along with their corresponding positions in the image to estimate the pose of the camera, and then the drone body. I do this by computing the homography matrix H for the points whose position we know in both the image plane as well as the real-world. The homography matrix converts points in the 3D world frame to the 2D image plane. Here, all the real-world points have Z coordinate as 0. Hence:\n\n\n\n\n\nWe get the Projective transformation equation as\n\n\n\n\n\nWe need 4 correspondences to constrain all 8 degrees of freedom. The elements are stored in row order In general, a Projective transformation can map any 4 points to any 4 points, with no triplets of collinear points. However, using more than 4 correspondences makes the results more robust. Stacking together all correspondences for a given time stamp we get the \\(A\\) matrix. Hence,\n\\[Ah = 0\\]\nWe perform least squares optimization by taking the SVD decomposition of matrix A.\n\n\n\n\n\nWe get the homography matrix from 9th coloumn of the V matrix.\n\n\n\n\n\nFrom this we extract our Rotation matrix \\((R)\\) and our Translation Vector \\((T)\\).\n\n\n\n\n\nThe diagonal guarantees it is a rotation matrix with determinant 1. To find our estimate of the translation we just make sure it is in the right scale using the following.\n\\[T = \\hat{T}/||\\hat{R_1}||\\]"
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html#pose-tracking-results",
    "href": "posts/5_RLAN_2_VO/index.html#pose-tracking-results",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "Pose Tracking Results",
    "text": "Pose Tracking Results\nThe results show that the calculated pose (blue) is closely tracking the ground truth from the Vicon (red).\n\n\n\nDataset 1: Position and Orientation Tracking Results\n\n\n\n\n\nDataset 2: Position and Orientation Tracking Results"
  },
  {
    "objectID": "posts/5_RLAN_2_VO/index.html#part-ii-velocity-estimation",
    "href": "posts/5_RLAN_2_VO/index.html#part-ii-velocity-estimation",
    "title": "Vision Based Pose and Velocity Estimation",
    "section": "Part II: Velocity Estimation",
    "text": "Part II: Velocity Estimation\n\nMethodology\nWe first extract corners in each image. I used MATLAB’s built in corner detectors detectFASTfeatures to accomplish this. The extracted corners includes corners from the random scribble.\nAfter extracting corners, the next step is to compute the motion(Optical Flow) between these corners in two consecutive images. This is achieved using the Kanade-Lucas-Tomasi (KLT) feature tracker in MATLAB. Optical flow refers to the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and the scene.\nWe need to solve the Motion Field Equation for the case of fixed depth. The Motion Field Equation relates the image plane velocities of the points (denoted by \\(( \\dot{p} )\\)) to the motion of the camera or object. This equation is based on the assumption of a fixed depth (denoted by \\(( Z )\\)), where$ ( A(p) )$ and \\(( B(p) )\\) are functions of the point coordinates in the image, and \\(( V )\\) and \\(( \\Omega )\\) represent the linear and angular velocities, respectively. \\[ \\dot{p} = \\frac{1}{Z} A(p)V + B(p)\\Omega\\] \\[\\dot{p} = \\frac{1}{Z} A(p)V + B(p)\\Omega = \\left( \\frac{1}{Z} A(p) \\quad B(p) \\right) \\begin{pmatrix} V \\\\ \\Omega \\end{pmatrix}\\]\nLeast Squares Minimization: The estimated velocities are obtained by solving an optimization problem, where the sum of the squared differences between the observed image plane velocities (( _i )) and those predicted by the Motion Field Equation is minimized. This is a classic least squares problem\n\\[V^*, \\Omega^* = \\arg\\min_{V,\\Omega} \\sum_{i=1}^n \\left\\| \\left( \\frac{1}{Z_i} A(p_i) \\quad B(p_i) \\right) \\begin{pmatrix} V \\\\ \\Omega \\end{pmatrix} - \\dot{p}_i \\right\\|^2\n\\]\nVelocity Estimation Equation: Finally, the solution to the optimization problem yields the estimated linear and angular velocities \\(( V^*)\\) and \\(( \\Omega^* )\\), encapsulated in a matrix form. The matrix $ ( H^+ )$ is the pseudo-inverse of a matrix \\(( H )\\), which relates the observed velocities \\(( \\dot{p} )\\) to the motion parameters \\(( V )\\) and \\(( \\Omega )\\). The pseudo-inverse is used when the system of equations is either underdetermined or overdetermined, allowing for a least squares solution to the problem.\n\\[\\begin{pmatrix}\nV^* \\\\\n\\Omega^*\n\\end{pmatrix} = H^+ \\dot{p}\\]\n\n\nRANSAC (RANdom SAmple Consensus)\nRANSAC is an algorithm used for estimating parameters of a mathematical model from a set of data that may contain outliers. The algorithm works by iteratively selecting random subsets of data points and fitting a model to these subsets. The model parameters are then evaluated on the remaining data points, and if the model fits well to a sufficient number of points, it is considered a good fit and the algorithm terminates. The number of attempts(k) required to achieve a probability of success Psuccess is given by the equation \\[k = \\frac{\\log(1 - P_{\\text{success}})}{\\log(1 - e^{-M})}\n\\]\nTo implement RANSAC I use the following pseudocode \n\n\nVeloctiy Estimation Results\nThe graph has a noticeable pattern of increased activity, particularly spikes, which could indicate rapid changes in altitude or disturbances affecting the drone’s movement. The predictions seems to be noisy need to experiment by tuning tracker parameters and using different tracker algorithms. Overall, the predictions track the true velocities closely but there is room for improvement.\n\n\n\nLinear and Angular Velocity Estimation Results\n\n\nNext Project: Unscented Kalman Filter"
  }
]